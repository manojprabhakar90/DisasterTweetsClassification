{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":17777,"databundleVersionId":869809,"sourceType":"competition"}],"dockerImageVersionId":30587,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"Introduction: \n\nIn this social media era, social media has become the most important medium through which information is getting conveyed. When a disaster occurs, it is important to know it immediately. Twitter is one platform which is being relied to provide authentic and genuine information. It is being monitored by Disaster teams as well as other agencies to keep informed on the disaster. There is an important point to be noted. The context. For example, \n\nKeyboard warriors are on fire has a different meaning when compared to the building is on fire. \n\nWe leverage machine learning to classify if the tweets are informing disaster or not. \n\nThe metric that is being validated is F1 Score\n\nTraining Data: 7613 records with 5 columns (id, keyword, location, text and target)\n\nTest Data: 3623 records with 4 columns (id, keyword, location, text)","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-11-30T07:39:24.243128Z","iopub.execute_input":"2023-11-30T07:39:24.243504Z","iopub.status.idle":"2023-11-30T07:39:24.692035Z","shell.execute_reply.started":"2023-11-30T07:39:24.243475Z","shell.execute_reply":"2023-11-30T07:39:24.690989Z"}}},{"cell_type":"markdown","source":"# Importing Libraries","metadata":{}},{"cell_type":"code","source":"\nimport re\nimport os\nimport string\nimport unicodedata\nfrom nltk.corpus import stopwords, wordnet\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom wordcloud import WordCloud\nfrom nltk.stem.porter import PorterStemmer\nfrom nltk.corpus import words\nword_dict = words.words()\nstemmer = PorterStemmer()\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import StratifiedKFold, KFold, GridSearchCV\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.svm import SVC\nfrom sklearn.metrics import accuracy_score, confusion_matrix, f1_score\nimport tensorflow as tf\nimport tensorflow_hub as hub\nfrom transformers import BertTokenizer, TFBertForSequenceClassification","metadata":{"execution":{"iopub.status.busy":"2023-11-30T16:43:37.590917Z","iopub.execute_input":"2023-11-30T16:43:37.591428Z","iopub.status.idle":"2023-11-30T16:43:57.399217Z","shell.execute_reply.started":"2023-11-30T16:43:37.591388Z","shell.execute_reply":"2023-11-30T16:43:57.398302Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Reading csv files","metadata":{}},{"cell_type":"code","source":"train_data = pd.read_csv(\"/kaggle/input/nlp-getting-started/train.csv\")","metadata":{"execution":{"iopub.status.busy":"2023-11-30T16:43:57.401103Z","iopub.execute_input":"2023-11-30T16:43:57.401894Z","iopub.status.idle":"2023-11-30T16:43:57.457413Z","shell.execute_reply.started":"2023-11-30T16:43:57.401858Z","shell.execute_reply":"2023-11-30T16:43:57.456159Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_data.head(5)","metadata":{"execution":{"iopub.status.busy":"2023-11-30T16:43:57.458484Z","iopub.execute_input":"2023-11-30T16:43:57.458752Z","iopub.status.idle":"2023-11-30T16:43:57.502608Z","shell.execute_reply.started":"2023-11-30T16:43:57.458728Z","shell.execute_reply":"2023-11-30T16:43:57.501552Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_data.shape","metadata":{"execution":{"iopub.status.busy":"2023-11-30T16:43:57.505853Z","iopub.execute_input":"2023-11-30T16:43:57.506621Z","iopub.status.idle":"2023-11-30T16:43:57.513303Z","shell.execute_reply.started":"2023-11-30T16:43:57.506585Z","shell.execute_reply":"2023-11-30T16:43:57.512293Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_data = pd.read_csv(\"/kaggle/input/nlp-getting-started/test.csv\")","metadata":{"execution":{"iopub.status.busy":"2023-11-30T16:43:57.514749Z","iopub.execute_input":"2023-11-30T16:43:57.515118Z","iopub.status.idle":"2023-11-30T16:43:57.547862Z","shell.execute_reply.started":"2023-11-30T16:43:57.515083Z","shell.execute_reply":"2023-11-30T16:43:57.547032Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_data.head(5)","metadata":{"execution":{"iopub.status.busy":"2023-11-30T16:43:57.548991Z","iopub.execute_input":"2023-11-30T16:43:57.549334Z","iopub.status.idle":"2023-11-30T16:43:57.559345Z","shell.execute_reply.started":"2023-11-30T16:43:57.549302Z","shell.execute_reply":"2023-11-30T16:43:57.558483Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_data.shape","metadata":{"execution":{"iopub.status.busy":"2023-11-30T16:43:57.560709Z","iopub.execute_input":"2023-11-30T16:43:57.561009Z","iopub.status.idle":"2023-11-30T16:43:57.569717Z","shell.execute_reply.started":"2023-11-30T16:43:57.560984Z","shell.execute_reply":"2023-11-30T16:43:57.568792Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# EDA and Text Preproessing","metadata":{}},{"cell_type":"code","source":"train_data.info()","metadata":{"execution":{"iopub.status.busy":"2023-11-30T16:43:57.570950Z","iopub.execute_input":"2023-11-30T16:43:57.571216Z","iopub.status.idle":"2023-11-30T16:43:57.602180Z","shell.execute_reply.started":"2023-11-30T16:43:57.571193Z","shell.execute_reply":"2023-11-30T16:43:57.601064Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_data.isnull().sum()","metadata":{"execution":{"iopub.status.busy":"2023-11-30T16:43:57.603504Z","iopub.execute_input":"2023-11-30T16:43:57.604113Z","iopub.status.idle":"2023-11-30T16:43:57.613765Z","shell.execute_reply.started":"2023-11-30T16:43:57.604079Z","shell.execute_reply":"2023-11-30T16:43:57.612723Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_data.info()","metadata":{"execution":{"iopub.status.busy":"2023-11-30T16:43:57.618267Z","iopub.execute_input":"2023-11-30T16:43:57.618671Z","iopub.status.idle":"2023-11-30T16:43:57.629089Z","shell.execute_reply.started":"2023-11-30T16:43:57.618641Z","shell.execute_reply":"2023-11-30T16:43:57.628009Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_data.isnull().sum()","metadata":{"execution":{"iopub.status.busy":"2023-11-30T16:43:57.630564Z","iopub.execute_input":"2023-11-30T16:43:57.630898Z","iopub.status.idle":"2023-11-30T16:43:57.648062Z","shell.execute_reply.started":"2023-11-30T16:43:57.630866Z","shell.execute_reply":"2023-11-30T16:43:57.647174Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Few Findings\n\nThere are 2533 (33%) of records are missing for location in training data. \n\nThere are 1105 (33%) of records are missing for location in test data\n\nKeyword - Approximately 1% of records are missing \n\nThere are no missing values in text and target","metadata":{}},{"cell_type":"markdown","source":"# We will replace missing values with Unknown for both keyword and location (Mode imputation will work as well)","metadata":{}},{"cell_type":"code","source":"train_data = train_data.fillna('Unknown')\ntest_data = test_data.fillna('Unknown')","metadata":{"execution":{"iopub.status.busy":"2023-11-30T16:43:57.649317Z","iopub.execute_input":"2023-11-30T16:43:57.649673Z","iopub.status.idle":"2023-11-30T16:43:57.661719Z","shell.execute_reply.started":"2023-11-30T16:43:57.649643Z","shell.execute_reply":"2023-11-30T16:43:57.660727Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_data=train_data[['keyword','location','text','target']].drop_duplicates() # There are 102 duplicates in training data. Dropping them","metadata":{"execution":{"iopub.status.busy":"2023-11-30T16:43:57.662616Z","iopub.execute_input":"2023-11-30T16:43:57.662856Z","iopub.status.idle":"2023-11-30T16:43:57.680956Z","shell.execute_reply.started":"2023-11-30T16:43:57.662831Z","shell.execute_reply":"2023-11-30T16:43:57.679753Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.displot(x = 'target', hue = 'target', data = train_data, palette = ['green', 'yellow'])\nplt.legend(['No Disaster', 'Disaster'])\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-11-30T16:43:57.682259Z","iopub.execute_input":"2023-11-30T16:43:57.682603Z","iopub.status.idle":"2023-11-30T16:43:58.529580Z","shell.execute_reply.started":"2023-11-30T16:43:57.682564Z","shell.execute_reply":"2023-11-30T16:43:58.528590Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_data['location'].value_counts()[0:20]","metadata":{"execution":{"iopub.status.busy":"2023-11-30T16:43:58.531071Z","iopub.execute_input":"2023-11-30T16:43:58.531703Z","iopub.status.idle":"2023-11-30T16:43:58.543870Z","shell.execute_reply.started":"2023-11-30T16:43:58.531670Z","shell.execute_reply":"2023-11-30T16:43:58.542872Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_data['keyword'].value_counts()[0:20]","metadata":{"execution":{"iopub.status.busy":"2023-11-30T16:43:58.545507Z","iopub.execute_input":"2023-11-30T16:43:58.546066Z","iopub.status.idle":"2023-11-30T16:43:58.555952Z","shell.execute_reply.started":"2023-11-30T16:43:58.546034Z","shell.execute_reply":"2023-11-30T16:43:58.554999Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# There are certain keywords which points to disaster. Also if we look at the location, most of the tweets are from US (If the location information is available)","metadata":{}},{"cell_type":"markdown","source":"# Its important to process text by removing stop words, non characters such as #, urls","metadata":{}},{"cell_type":"code","source":"def clean_txt(txt):\n    res = unicodedata.normalize('NFKC', txt)\n    res = re.sub(r'[^\\x00-\\x7F]+', r'', res)\n    res = re.sub(r'^RT[\\s]+', r'', res)\n    res = re.sub(r'\\$\\w*', r'', res)\n    res = re.sub(r'&lt;', r'<', res)\n    res = re.sub(r'&gt;', r'>', res)\n    res = re.sub(r'&amp;?', r'and', res)\n    res = re.sub(r'<[^>]*?>', r'', res)\n    res = re.sub(r'#', r' #', res)\n    res = re.sub(r'\\s#\\s', r' ', res)\n    return res\n\ntrain_data['text'] = train_data['text'].apply(clean_txt )","metadata":{"execution":{"iopub.status.busy":"2023-11-30T16:43:58.557465Z","iopub.execute_input":"2023-11-30T16:43:58.558097Z","iopub.status.idle":"2023-11-30T16:43:58.713775Z","shell.execute_reply.started":"2023-11-30T16:43:58.558064Z","shell.execute_reply":"2023-11-30T16:43:58.713090Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"text = \" \".join(review for review in train_data['text'])\nwordcloud = WordCloud(background_color=\"white\").generate(text)\nplt.figure(figsize=(10, 5))\nplt.imshow(wordcloud, interpolation='bilinear')\nplt.axis(\"off\")\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-11-30T16:43:58.715031Z","iopub.execute_input":"2023-11-30T16:43:58.715634Z","iopub.status.idle":"2023-11-30T16:44:00.104584Z","shell.execute_reply.started":"2023-11-30T16:43:58.715599Z","shell.execute_reply":"2023-11-30T16:44:00.103350Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pd.Series(' '.join(train_data['text']).lower().split()).value_counts()[:50] # Top 50 words","metadata":{"execution":{"iopub.status.busy":"2023-11-30T16:44:00.106057Z","iopub.execute_input":"2023-11-30T16:44:00.106947Z","iopub.status.idle":"2023-11-30T16:44:00.169663Z","shell.execute_reply.started":"2023-11-30T16:44:00.106911Z","shell.execute_reply":"2023-11-30T16:44:00.168646Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Data Cleaning","metadata":{}},{"cell_type":"code","source":"stop_words = set(stopwords.words('english'))\ndef text_processing(text):\n    words = text.lower().split()\n    filtered_words = [word for word in words if word not in stop_words]\n    clean_text = ' '.join(filtered_words)\n    clean_text = clean_text.translate(str.maketrans('', '', string.punctuation)).strip()\n    return clean_text","metadata":{"execution":{"iopub.status.busy":"2023-11-30T16:44:00.171026Z","iopub.execute_input":"2023-11-30T16:44:00.171849Z","iopub.status.idle":"2023-11-30T16:44:00.181102Z","shell.execute_reply.started":"2023-11-30T16:44:00.171813Z","shell.execute_reply":"2023-11-30T16:44:00.180070Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_data['text'] = train_data['text'].apply(text_processing)","metadata":{"execution":{"iopub.status.busy":"2023-11-30T16:44:00.182599Z","iopub.execute_input":"2023-11-30T16:44:00.182926Z","iopub.status.idle":"2023-11-30T16:44:00.262550Z","shell.execute_reply.started":"2023-11-30T16:44:00.182879Z","shell.execute_reply":"2023-11-30T16:44:00.261619Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"text = \" \".join(review for review in train_data['text'])\nwordcloud = WordCloud(background_color=\"white\").generate(text)\nplt.figure(figsize=(10, 5))\nplt.imshow(wordcloud, interpolation='bilinear')\nplt.axis(\"off\")\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-11-30T16:44:00.263769Z","iopub.execute_input":"2023-11-30T16:44:00.264149Z","iopub.status.idle":"2023-11-30T16:44:01.753528Z","shell.execute_reply.started":"2023-11-30T16:44:00.264122Z","shell.execute_reply":"2023-11-30T16:44:01.752657Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pd.Series(' '.join(train_data['text']).lower().split()).value_counts()[:50] # Top 50 words","metadata":{"execution":{"iopub.status.busy":"2023-11-30T16:44:01.754659Z","iopub.execute_input":"2023-11-30T16:44:01.755016Z","iopub.status.idle":"2023-11-30T16:44:01.798747Z","shell.execute_reply.started":"2023-11-30T16:44:01.754989Z","shell.execute_reply":"2023-11-30T16:44:01.797669Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Model 1","metadata":{}},{"cell_type":"markdown","source":"# Splitting training data into 70:30","metadata":{}},{"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split(train_data['text'], train_data['target'], test_size=0.2, random_state=42)","metadata":{"execution":{"iopub.status.busy":"2023-11-30T16:44:01.799971Z","iopub.execute_input":"2023-11-30T16:44:01.801036Z","iopub.status.idle":"2023-11-30T16:44:01.808255Z","shell.execute_reply.started":"2023-11-30T16:44:01.801001Z","shell.execute_reply":"2023-11-30T16:44:01.807329Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train.shape, y_train.shape, X_test.shape, y_test.shape","metadata":{"execution":{"iopub.status.busy":"2023-11-30T16:44:01.809541Z","iopub.execute_input":"2023-11-30T16:44:01.809871Z","iopub.status.idle":"2023-11-30T16:44:01.819422Z","shell.execute_reply.started":"2023-11-30T16:44:01.809834Z","shell.execute_reply":"2023-11-30T16:44:01.818353Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\nmodel = TFBertForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=1)","metadata":{"execution":{"iopub.status.busy":"2023-11-30T16:44:01.820891Z","iopub.execute_input":"2023-11-30T16:44:01.821247Z","iopub.status.idle":"2023-11-30T16:44:17.754582Z","shell.execute_reply.started":"2023-11-30T16:44:01.821184Z","shell.execute_reply":"2023-11-30T16:44:17.753703Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train1=X_train.tolist()\nX_test1=X_test.tolist()\nX_train_encoded = tokenizer(X_train1, padding=True, truncation=True, return_tensors=\"tf\")\nX_test_encoded = tokenizer(X_test1, padding=True, truncation=True, return_tensors=\"tf\")\ntrain_dataset = tf.data.Dataset.from_tensor_slices((dict(X_train_encoded), y_train)).shuffle(100).batch(32)\ntest_dataset = tf.data.Dataset.from_tensor_slices((dict(X_test_encoded), y_test)).batch(32)","metadata":{"execution":{"iopub.status.busy":"2023-11-30T16:44:17.764878Z","iopub.execute_input":"2023-11-30T16:44:17.765209Z","iopub.status.idle":"2023-11-30T16:44:22.924981Z","shell.execute_reply.started":"2023-11-30T16:44:17.765175Z","shell.execute_reply":"2023-11-30T16:44:22.924189Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"loss_object = tf.keras.losses.BinaryCrossentropy(from_logits=True) \noptimizer = tf.keras.optimizers.Adam(learning_rate=2e-5)\ntrain_loss = tf.keras.metrics.Mean(name='train_loss')\ntrain_accuracy = tf.keras.metrics.BinaryAccuracy(name='train_accuracy')\nepochs = 3 \nfor epoch in range(epochs):\n    train_loss.reset_states()\n    train_accuracy.reset_states()\n\n    for batch_inputs, batch_labels in train_dataset:\n        with tf.GradientTape() as tape:\n            outputs = model(batch_inputs, training=True).logits\n            loss = loss_object(batch_labels, outputs)\n\n        gradients = tape.gradient(loss, model.trainable_variables)\n        optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n\n        train_loss(loss)\n        train_accuracy(batch_labels, tf.sigmoid(outputs))  # Apply sigmoid activation for accuracy calculation\n\n    print(f\"Epoch {epoch + 1}: Loss {train_loss.result()}, Accuracy {train_accuracy.result()}\")\n\n# Evaluation\ntest_accuracy = tf.keras.metrics.BinaryAccuracy(name='test_accuracy')  \n\nfor batch_inputs, batch_labels in test_dataset:\n    test_predictions = model(batch_inputs, training=False).logits\n    test_accuracy(batch_labels, tf.sigmoid(test_predictions))\n\nprint(f\"Test Accuracy: {test_accuracy.result()}\")\n","metadata":{"execution":{"iopub.status.busy":"2023-11-30T16:44:22.926032Z","iopub.execute_input":"2023-11-30T16:44:22.926289Z","iopub.status.idle":"2023-11-30T16:52:14.594358Z","shell.execute_reply.started":"2023-11-30T16:44:22.926254Z","shell.execute_reply":"2023-11-30T16:52:14.593335Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Model 2 - With a different learning rate","metadata":{}},{"cell_type":"code","source":"loss_object = tf.keras.losses.BinaryCrossentropy(from_logits=True) \noptimizer = tf.keras.optimizers.Adam(learning_rate=1e-5)\ntrain_loss = tf.keras.metrics.Mean(name='train_loss')\ntrain_accuracy = tf.keras.metrics.BinaryAccuracy(name='train_accuracy')\nepochs = 3 \nfor epoch in range(epochs):\n    train_loss.reset_states()\n    train_accuracy.reset_states()\n\n    for batch_inputs, batch_labels in train_dataset:\n        with tf.GradientTape() as tape:\n            outputs = model(batch_inputs, training=True).logits\n            loss = loss_object(batch_labels, outputs)\n\n        gradients = tape.gradient(loss, model.trainable_variables)\n        optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n\n        train_loss(loss)\n        train_accuracy(batch_labels, tf.sigmoid(outputs))  # Apply sigmoid activation for accuracy calculation\n\n    print(f\"Epoch {epoch + 1}: Loss {train_loss.result()}, Accuracy {train_accuracy.result()}\")\n\n# Evaluation\ntest_accuracy = tf.keras.metrics.BinaryAccuracy(name='test_accuracy')  \n\nfor batch_inputs, batch_labels in test_dataset:\n    test_predictions = model(batch_inputs, training=False).logits\n    test_accuracy(batch_labels, tf.sigmoid(test_predictions))\n\nprint(f\"Test Accuracy: {test_accuracy.result()}\")\n","metadata":{"execution":{"iopub.status.busy":"2023-11-30T16:53:26.291074Z","iopub.execute_input":"2023-11-30T16:53:26.291481Z","iopub.status.idle":"2023-11-30T17:01:18.319173Z","shell.execute_reply.started":"2023-11-30T16:53:26.291450Z","shell.execute_reply":"2023-11-30T17:01:18.318129Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Apply preprocessing - The same as the one in training data","metadata":{}},{"cell_type":"code","source":"test_data['text'] = test_data['text'].apply(clean_txt )\ntest_data['text'] = test_data['text'].apply(text_processing)","metadata":{"execution":{"iopub.status.busy":"2023-11-30T17:01:32.146040Z","iopub.execute_input":"2023-11-30T17:01:32.146945Z","iopub.status.idle":"2023-11-30T17:01:32.235156Z","shell.execute_reply.started":"2023-11-30T17:01:32.146910Z","shell.execute_reply":"2023-11-30T17:01:32.234253Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Use the best model for classification - Second model","metadata":{}},{"cell_type":"code","source":"encoded_texts = tokenizer(list(test_data[\"text\"]), padding=True, truncation=True, return_tensors=\"tf\")\n\ndataset = tf.data.Dataset.from_tensor_slices((dict(encoded_texts)))\npredictions = []\n\nfor batch_inputs in dataset.batch(32):\n    batch_predictions = model(batch_inputs, training=False).logits\n    batch_probabilities = tf.sigmoid(batch_predictions)\n    batch_labels = [1 if p >= 0.5 else 0 for p in batch_probabilities]\n    predictions.extend(batch_labels)\n\n","metadata":{"execution":{"iopub.status.busy":"2023-11-30T17:02:16.558696Z","iopub.execute_input":"2023-11-30T17:02:16.559542Z","iopub.status.idle":"2023-11-30T17:02:41.765744Z","shell.execute_reply.started":"2023-11-30T17:02:16.559498Z","shell.execute_reply":"2023-11-30T17:02:41.764721Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"len(predictions)","metadata":{"execution":{"iopub.status.busy":"2023-11-30T17:02:41.767260Z","iopub.execute_input":"2023-11-30T17:02:41.767586Z","iopub.status.idle":"2023-11-30T17:02:41.773723Z","shell.execute_reply.started":"2023-11-30T17:02:41.767560Z","shell.execute_reply":"2023-11-30T17:02:41.772742Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission = pd.read_csv(\"/kaggle/input/nlp-getting-started/sample_submission.csv\")\nsubmission['target'] = predictions","metadata":{"execution":{"iopub.status.busy":"2023-11-30T17:02:44.155232Z","iopub.execute_input":"2023-11-30T17:02:44.155653Z","iopub.status.idle":"2023-11-30T17:02:44.172455Z","shell.execute_reply.started":"2023-11-30T17:02:44.155621Z","shell.execute_reply":"2023-11-30T17:02:44.171495Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission.head(5)","metadata":{"execution":{"iopub.status.busy":"2023-11-30T17:02:52.904663Z","iopub.execute_input":"2023-11-30T17:02:52.905041Z","iopub.status.idle":"2023-11-30T17:02:52.914111Z","shell.execute_reply.started":"2023-11-30T17:02:52.905009Z","shell.execute_reply":"2023-11-30T17:02:52.913061Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission.to_csv(\"submission.csv\",index=False)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Conclusion\n\n1. Started with TF-IDF, it didnt give a great result on the test daata. So moved to Transformers,  \n\n2. BERT is Bidirectional Encoder Representations Transformers. \n\n3. BERT is built upon the Transformer architecture, which was introduced in the paper \"Attention Is All You Need\" by Vaswani et al. Transformers have proven to be highly effective in handling sequential data, making them well-suited for NLP tasks. \n\n4. Unlike previous NLP models like RNN or LSTM or GRU, which processed information in unidirection (either right to left or left to right). BERT can be used bidirectional. \n\n5. BERT has been used across different NLP tasks such as classification, summarization. \n\n6. The accuracy of these models is hovering around 0.82. \n\n7. Other models such as Deberta, Roberta can be used to check how the performance of these models are. \n","metadata":{}}]}